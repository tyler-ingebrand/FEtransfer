<!DOCTYPE html>
<html>
<head>
    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5BNW4QLTSC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5BNW4QLTSC');
  </script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="This project introduces a geometric framework for transfer learning in Hilbert spaces, defining three types of inductive transfer. Furthermore, we generalize the function encoder algorithm to the full Hilbert space setting, prove a universal function space approximation theorem, and demonstrate that the algorithm outperforms state.">
  <meta property="og:title" content="Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces"/>
  <meta property="og:description" content="We introduce a novel approach to transfer learning in Hilbert spaces, using function encoders to achieve interpolation, extrapolation, and generalization across tasks. Our method outperforms state-of-the-art techniques like transformers and meta-learning on multiple benchmarks, offering a new path for fast adaptation in machine learning.">
  <meta property="og:url" content="https://tyler-ingebrand.github.io/FunctionEncoderTran"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/ICML2025_cover.png" />
  <meta property="og:image:width" content="2000"/>
  <meta property="og:image:height" content="1400"/>


  <meta name="twitter:title" content="Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces">
  <meta name="twitter:description" content="We introduce a novel approach to transfer learning in Hilbert spaces, using function encoders to achieve interpolation, extrapolation, and generalization across tasks. Our method outperforms state-of-the-art techniques like transformers and meta-learning on multiple benchmarks, offering a new path for fast adaptation in machine learning.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/ICML2025_cover.png">
  <meta name="twitter:card" content="static/images/ICML2025_cover.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="function encoder, basis functions, transfer learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Function Encoders:<br> A Principled Approach to Transfer Learning in Hilbert Spaces</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tyler-ingebrand.github.io/" target="_blank">Tyler Ingebrand</a>,</span>
                <span class="author-block">
                  <a href="https://ajthor.github.io/" target="_blank">Adam J. Thorpe</a>,</span>
                  <span class="author-block">
                    <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu" target="_blank">Ufuk Topcu</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The University of Texas at Austin<br>Under Review</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (Soon)</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/tyler-ingebrand/FEtransfer" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- abstract -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="margin-bottom: 40px;">
            A central challenge in transfer learning is designing algorithms that can quickly adapt and generalize to new tasks without retraining. Yet, the conditions of when and how algorithms can effectively transfer to new tasks is poorly characterized. We introduce a geometric characterization of transfer in Hilbert spaces and define three types of inductive transfer: interpolation within the convex hull, extrapolation to the linear span, and extrapolation outside the span. We propose a method grounded in the theory of function encoders to achieve all three types of transfer. Specifically, we introduce a novel training scheme for function encoders using least-squares optimization, prove a universal approximation theorem for function encoders, and provide a comprehensive comparison with existing approaches such as transformers and meta-learning on four diverse benchmarks. Our experiments demonstrate that the function encoder outperforms state-of-the-art methods on four benchmark tasks and on all three types of transfer.
            <br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- abstract -->

<!-- image -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
         <h2 class="title is-3">A Geometric Characterization of Inductive Transfer</h2>
         <div class="content has-text-justified">
          <img src="static/images/ICML2025_cover_no_background.png" alt="An illustration of the three types of transfer. Type 1 transfer is within the convex hull of the training functions. Type 2 transfer is extrapolation to the linear span. Type 3 transfer is extrapolation to the rest of the Hilbert space. "/>
          <p style="margin-bottom: 5px;">
            We consider an inductive transfer setting and present a geometric characterization of inductive transfer using principles from functional analysis.
            Inductive transfer involves transferring knowledge to new, unseen tasks while keeping the data distribution the same.
            For instance, labeling images according to a new, previously unknown class, where only a few examples are provided after training.
            While prior works have studied transfer learning, gaps remain in identifying when learned models will succeed and when they will fail.
            We introduce a characterization of inductive transfer, based on Hilbert spaces, which will provide intuition about the difficulty of a given transfer learning problem.
          </p >
          <p style="margin-bottom: 1px;">
            Specifically, we characterize transfer using three types:
          </p>
           <p>
            <p style="margin-left: 20px;margin-bottom: 1px;"><strong>(Type 1) Interpolation within the convex hull.</strong> Tasks that can be represented as a convex combination of observed source tasks. </p>
            <br>
            <p style="margin-left: 20px;margin-bottom: 1px;"><strong>(Type 2) Extrapolation to the linear span.</strong> Tasks that are in the linear span of source tasks, which may lie far from observed data but share meaningful features. </p>
            <br>
            <p style="margin-left: 20px;margin-bottom: 15px;"><strong>(Type 3) Extrapolation to the Hilbert space.</strong> Tasks that are outside the linear span of the source predictors in an infinite-dimensional function space. Type 3 transfer is the most important and challenging form of transfer. </p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- image -->


  <!-- video -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
       <h2 class="title is-3">The Function Encoder</h2>
        <div class="content has-text-justified">
         <img src="static/images/function_encoder.gif" width="100%" alt="A video of learned basis functions converging to span the space of quadratic functions."/>
         <div class="content has-text-justified">
          <p>

            Basis functions are a natural solution to inductive transfer learning in a Hilbert space.
            We build upon the function encoder algorithm, a method for learning neural network basis functions to span an arbitrary function space.
            While analytical approaches such as Fourier series scale poorly with the dimensionality of the input and output spaces, function encoders scale extremely well due to the use of neural networks.
          </p>
          <p>
            We make several improvements to the function encoder algorithm.
            First, we generalize all definitions to use inner products only, allowing the function encoder to work on any Hilbert space.
            This generalization allows us to tackle new problems, such as few-shot classification.
            Second, we introduce a novel training scheme for function encoders using least-squares optimization. This training scheme greatly improves convergence rate and accuracy.
            Lastly, we prove a universal function space approximation theorem for function encoders, showing that they can approximate any function in a separable Hilbert space to any desired accuracy.
          </p>
         </div>
        </div>
      </div>
  </div>
</section>



<!-- Image carousel -->
<section class="hero is-small is-light">
   <div class="columns is-centered has-text-centered">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Experimental Results</h2>
      <div id="results-carousel" class="carousel results-carousel">


        <div class="item">
        <img src="static/images/canonical (1)_no_background.png" alt="An illustration of a function encoder with 100 basis functions approximating a type 3 transfer funciton."/>
        <h2 class="subtitle has-text-justified">
          <strong>Qualitative Analysis of Transfer on the Polynomial Dataset.</strong>
            In this illustrative example, we visualize the function encoder and one baseline, the auto encoder, on each of the three types of transfer.
            We observe that both approaches achieve reasonable performance for type 1 transfer.
            For type 2 transfer, the target function is much larger in magnitude than any function in the training set.
            The auto encoder fails at this function because it has only learned to output functions from the training function space.
            In contrast, the function encoder generalizes to the entire span of the training function space by design.
            For type 3 transfer, the target function is a cubic function.
            The auto encoder nonetheless outputs a function that is similar to the ones seen during training.
            When using a function encoder with only three basis functions, the basis functions only span the three-dimensional space of quadratic functions, and so its approximation is the best quadratic to fit the data.
            When using 100 basis functions, the basis functions spans the space of quadratics, but additionally have 97 unconstrained dimensions.
            Due to the use of least squares, the function encoder with 100 basis functions optimally uses these extra 97 dimensions to fit the new function.
            Therefore, it is able to reasonable approximate this function as well, despite having never seen a cubic function during training.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/poly (1)_no_background.png" alt="The training curves for all baselines on the polynomial dataset. The function encoder outperforms all baselines by orders of magnitude. "/>
        <h2 class="subtitle has-text-justified">
         <strong>Empirical Results on the Polynomial Dataset.</strong>
          While many approaches demonstrate moderate type 1 transfer, only the function encoder successfully achieves all three types,
          as illustrated by its orders of magnitude advantage over other approaches.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/cifar (1)_no_background.png" alt="The training curves for all baselines on the CIFAR dataset. The function encoder outperforms all baselines, including ad-hoc approaches such as Siamese networks."/>
      <h2 class="subtitle has-text-justified">
        <strong>Empirical Results on the CIFAR Dataset.</strong>
        The training curves show the two ad-hoc baselines seem to be performing best, and many algorithms fail to converge on all or some seeds.
        However, when measuring type 1 transfer, the function encoder performs best, achieving slightly better performance than Siamese networks.
        For type 3 transfer, few-shot classification of unseen classes, the function encoder again performs best, albeit similar to Siamese networks.
        The key idea is that function encoders are performing comparably to ad-hoc approaches despite being designed for a more general setting.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{ingebrand_2025_fe_transfer,
  author       = {Tyler Ingebrand and
                  Adam J. Thorpe and
                  Ufuk Topcu},
  title        = {Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces},
  year         = {2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
