<!DOCTYPE html>
<html>
<head>
    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5BNW4QLTSC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5BNW4QLTSC');
  </script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="This project introduces a geometric framework for transfer learning in Hilbert spaces, defining three types of inductive transfer. Furthermore, we generalize the function encoder algorithm to the full Hilbert space setting, prove a universal function space approximation theorem, and demonstrate that the algorithm outperforms state.">
  <meta property="og:title" content="Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces"/>
  <meta property="og:description" content="We introduce a novel approach to transfer learning in Hilbert spaces, using function encoders to achieve interpolation, extrapolation, and generalization across tasks. Our method outperforms state-of-the-art techniques like transformers and meta-learning on multiple benchmarks, offering a new path for fast adaptation in machine learning.">
  <meta property="og:url" content="https://tyler-ingebrand.github.io/FunctionEncoderTran"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/ICML2025_cover.png" />
  <meta property="og:image:width" content="2000"/>
  <meta property="og:image:height" content="1400"/>


  <meta name="twitter:title" content="Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces">
  <meta name="twitter:description" content="We introduce a novel approach to transfer learning in Hilbert spaces, using function encoders to achieve interpolation, extrapolation, and generalization across tasks. Our method outperforms state-of-the-art techniques like transformers and meta-learning on multiple benchmarks, offering a new path for fast adaptation in machine learning.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/ICML2025_cover.png">
  <meta name="twitter:card" content="static/images/ICML2025_cover.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="function encoder, basis functions, transfer learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Function Encoders:<br> A Principled Approach to Transfer Learning in Hilbert Spaces</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tyler-ingebrand.github.io/" target="_blank">Tyler Ingebrand</a>,</span>
                <span class="author-block">
                  <a href="https://ajthor.github.io/" target="_blank">Adam J. Thorpe</a>,</span>
                  <span class="author-block">
                    <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu" target="_blank">Ufuk Topcu</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The University of Texas at Austin<br>ICML 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2501.18373" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/tyler-ingebrand/FEtransfer" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.18373" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- abstract -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="margin-bottom: 40px;">
            A central challenge in transfer learning is designing algorithms that can quickly adapt and generalize to new tasks without retraining. Yet, the conditions of when and how algorithms can effectively transfer to new tasks is poorly characterized. We introduce a geometric characterization of transfer in Hilbert spaces and define three types of inductive transfer: interpolation within the convex hull, extrapolation to the linear span, and extrapolation outside the span. We propose a method grounded in the theory of function encoders to achieve all three types of transfer. Specifically, we introduce a novel training scheme for function encoders using least-squares optimization, prove a universal approximation theorem for function encoders, and provide a comprehensive comparison with existing approaches such as transformers and meta-learning on four diverse benchmarks. Our experiments demonstrate that the function encoder outperforms state-of-the-art methods on four benchmark tasks and on all three types of transfer.
            <br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- abstract -->

<!-- image -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
         <h2 class="title is-3">A Geometric Characterization of Inductive Transfer</h2>
         <div class="content has-text-justified">
          <img src="static/images/ICML2025_cover_no_background.png" alt="An illustration of the three types of transfer. Type 1 transfer is within the convex hull of the training functions. Type 2 transfer is extrapolation to the linear span. Type 3 transfer is extrapolation to the rest of the Hilbert space. "/>
          <p style="margin-bottom: 5px;">
            We consider an inductive transfer setting and present a geometric characterization of inductive transfer using principles from functional analysis.
            Inductive transfer involves transferring knowledge to new, unseen tasks while keeping the data distribution the same.
            For instance, labeling images according to a new, previously unknown class, where only a few examples are provided after training.
            While prior works have studied transfer learning, gaps remain in identifying when learned models will succeed and when they will fail.
            We introduce a characterization of inductive transfer, based on Hilbert spaces, which will provide intuition about the difficulty of a given transfer learning problem.
          </p >
          <p style="margin-bottom: 1px;">
            Specifically, we characterize transfer using three types:
          </p>
           <p>
            <p style="margin-left: 20px;margin-bottom: 1px;"><strong>(Type 1) Interpolation within the convex hull.</strong> Tasks that can be represented as a convex combination of observed source tasks. </p>
            <br>
            <p style="margin-left: 20px;margin-bottom: 1px;"><strong>(Type 2) Extrapolation to the linear span.</strong> Tasks that are in the linear span of source tasks, which may lie far from observed data but share meaningful features. </p>
            <br>
            <p style="margin-left: 20px;margin-bottom: 15px;"><strong>(Type 3) Extrapolation to the Hilbert space.</strong> Tasks that are outside the linear span of the source predictors in an infinite-dimensional function space. Type 3 transfer is the most important and challenging form of transfer. </p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- image -->


  <!-- video -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
       <h2 class="title is-3">The Function Encoder</h2>
        <div class="content has-text-justified">
        <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/images/FunctionEncoderConcepts.mp4"
            type="video/mp4">
          </video>
          <div class="content has-text-justified">
          <br>
             <p>

            Basis functions are a natural solution to inductive transfer learning in a Hilbert space.
            We build upon the function encoder algorithm, a method for learning neural network basis functions to span an arbitrary function space.
            While analytical approaches such as Fourier series scale poorly with the dimensionality of the input and output spaces, function encoders scale extremely well due to the use of neural networks.
          </p>
          <p>
            We make several improvements to the function encoder algorithm.
            First, we generalize all definitions to use inner products only, allowing the function encoder to work on any Hilbert space.
            This generalization allows us to tackle new problems, such as few-shot classification.
            Second, we introduce a novel training scheme for function encoders using least-squares optimization. This training scheme greatly improves convergence rate and accuracy.
            Lastly, we prove a <strong>universal function space approximation theorem</strong> for function encoders, showing that they can approximate any function in a separable Hilbert space to any desired accuracy.
          </p>
         </div>
        </div>
      </div>
  </div>
</section>


<!--   <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--<div class="content has-text-justified">-->
<!-- Image carousel -->


<section class="hero is-small is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <div class="content has-text-justified" >
                    <h2 class="title is-3 has-text-centered" style="margin-top: 10px;">Experimental Results</h2>

                    <div class="content has-text-justified">
                    We compare function encoders against state-of-the-art transfer learning algorithms on four different datasets.
                    The first is simple polynomial regression, and this example is designed to highlight the failure modes of other approaches.
                    The other three datasets are more challenging: Few-shot image classification for CIFAR, Pose estimation on the seven scenes dataset, and dynamics prediction for a MuJoCo ant.
                    For each dataset, we also separately evaluate each type of transfer, and we show that the function encoder outperforms prior works, such as transformers, meta learning, and auto-encoders,  on all three types of transfer.
                    </div>

                    <h3 class="subtitle is-5 "> An Illustrative Polynomial Example</h3>

                    <div class="content has-text-justified">
                      In this example, the algorithms estimate a polynomial function from a few data points. The training set consists of quadratic functions.
                      Type 1 transfer are function sampled from the same distribution as the training set.
                      Type 2 transfer are functions that are much larger in magnitude then anything in the training set, but still quadratic.
                      Type 3 transfer is cubic functions.
                      To illustrate this, we provide a simple qualitative example comparing the function encoder against auto-encoders.
                    </div>

                    <img src="static/images/canonical (1)_no_background.png" alt="An illustration of a function encoder with 100 basis functions approximating a type 3 transfer funciton."/>

                    <div class="content has-text-justified">
                       We observe that both approaches achieve reasonable performance for type 1 transfer.
                       For type 2 transfer, the target function is much larger in magnitude than any function in the training set.
                       The auto encoder fails at this function because it has only learned to output functions from the training function space.
                       In contrast, the function encoder generalizes to the entire span of the training function space by design.
                       For type 3 transfer, the target function is a cubic function.
                       The auto encoder nonetheless outputs a function that is similar to the ones seen during training.
                       When using a function encoder with only three basis functions, the basis functions only span the three-dimensional space of quadratic functions, and so its approximation is the best quadratic to fit the data.
                       When using 100 basis functions, the basis functions spans the space of quadratics, but additionally have 97 unconstrained dimensions.
                       Due to the use of least squares, the function encoder with 100 basis functions optimally uses these extra 97 dimensions to fit the new function.
                       Therefore, it is able to reasonable approximate this function as well, despite having never seen a cubic function during training.

                       The quantitative results are shown here:
                    </div>

                    <img src="static/images/poly (1)_no_background.png" alt="The training curves for all baselines on the polynomial dataset. The function encoder outperforms all baselines by orders of magnitude. "/>

                    <div class="content has-text-justified" style="margin-top: 10px;">
                       All prior works such as meta-learning and transformers fail on type 2 and type 3 transfer, even for a simple 1-dimensional polynomial regression problem.
                    </div>

                    <h3 class="subtitle is-5 "> Few-Shot Image Classification</h3>

                    <div class="content has-text-justified">
                      Due to the broad applicability of Hilbert spaces, we can apply the function encoder to many settings.
                      To highlight this, we apply it to the few-shot image classification problem on the CIFAR dataset.
                      In this setting, the model is given some positive examples showing what the class looks like. It is also given some negative examples of images belonging to other classes.
                      Then, for any new class, the model should predict if the specified class is present in the image.
                    </div>

                    <img src="static/images/FewShotImageClass.png" alt="A visualization of the few-shot image classification problem. Some positive examples of a desired class are provided, along with some negative examples of other classes. The model should be able to identify the class for any new image.  "/>


                    <div class="content has-text-justified" style="margin-top: 10px;">
                      We use 90 of the 100 classes for training. Type 1 transfer consists of unseen images from these 90 classes.
                      Type 2 transfer is not easily testable, but would consists of images that belong to two classes at once.
                      Type 3 transfer is the 10, unseen classes.
                      The training results are shown below:
                    </div>

                    <img src="static/images/cifar (1)_no_background.png" alt="The training curves for all baselines on the CIFAR dataset. The function encoder outperforms all baselines, including ad-hoc approaches such as Siamese networks."/>

                    <div class="content has-text-justified" style="margin-top: 10px;">
                      In addition to the typical baselines, we also compare against Siamese networks. The results show that function encoders perform
                      even better than Siamese networks on this problem, despite the fact that Siamese networks are designed explicitly for this setting.
                      All other approaches perform poorly, which highlights the difficulty of this problem.
                    </div>

                    <h3 class="subtitle is-5"> Pose Estimation</h3>

                    <div class="content has-text-justified" style="margin-top: 10px;">
                      Another interesting problem that can be expressed as a Hilbert space is pose estimation.
                      The model is provided with a set of images and the location of the camera when these images were taken.
                      Then, for any new image, the model should predict the location of the camera.
                    </div>

                    <img src="static/images/PoseEstim.png" alt="The model is provided with a set of images and their locations. For any new image, it should estimate the location of the camera. "/>

                    <div class="content has-text-justified" style="margin-top: 10px;">
                      We use the 7 scenes dataset, which consists of 7 different scenes. The model is trained on 6 of these scenes. Unseen images from these 6 scenes are
                      used for type 1 transfer. Type 2 transfer would consist of shifting the origin or scaling the units. The seventh scene is used for type 3 transfer.
                    </div>

                    <img src="static/images/seven (1)_no_background.png" alt="The training curves for all baselines on the 7 Scenes dataset. The function encoder outperforms all baselines."/>

                    <div class="content has-text-justified" style="margin-top: 10px;">
                    Many approaches converge during training. As expected, all approaches perform much worse at type 1 transfer, indicating a degree of over-fitting. The function encoder performs best at both type 1 and type 3 transfer, indicating its ability to optimally use the learned features for unseen data.
                    </div>

                    <h3 class="subtitle is-5"> Hidden-Parameter Dynamics Estimation for MuJoCo</h3>

                    <div class="content has-text-justified" style="margin-top: 10px;">
                      Lastly, we run experiments on a hidden-parameter version of the MuJoCo Ant. The algorithm is given data
                      from the beginning of a trajectory, and it must estimate the dynamics going forward. The training dataset consists of small robots.
                      Type 1 transfer consists of robots sampled from the same hidden-parameter distribution as training. Type 2 transfer is evaluated via
                      synthetically generated data consisting of the linear combination of the dynamics present in type 1. Type 3 transfer are robots that are much larger in size than the training set.
                      We visualize these robots below:

                    </div>

                    <img src="static/images/collage (1).png" alt="Type 1 transfer consists of small, 4 legged robots. Type 3 transfer consists of very large, 4 legged robots."/>
                    <img src="static/images/ant (1)_no_background.png" alt="The training curves for all baselines on the ant dataset. The function encoder outperforms all baselines."/>

                    <div class="content has-text-justified" style="margin-top: 10px; margin-bottom: 15px;">
                      All algorithms demonstrate convergence during training, albeit to various levels. However, many algorithms perform much worse for type 1 transfer.
                      The function encoder performs best, although many approaches, such as the transformer and MAML(n=5), are comparable.
                      Furthermore, the function encoder is clearly best for type 2 transfer. Type 3 transfer tells an interesting story.
                      The function encoder has the best stable performance, although approaches such as the transformer and the auto encoder are not far behind.
                      At the beginning of training, the auto encoder shows the best overall performance, although it degrades as training continues.
                      This is because training is not optimizing for type 3 transfer, and the best model parameters for the training dataset are not the best model parameters for type 3 transfer.
                      Thus, its performance is unstable.
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>











<!-- End image carousel -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{ingebrand_2025_fe_transfer,
  author       = {Tyler Ingebrand and
                  Adam J. Thorpe and
                  Ufuk Topcu},
  title        = {Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces},
  year         = {2025}
  journal      = {International Conference on Machine Learning (ICML)},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
